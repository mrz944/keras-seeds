{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.callbacks import CSVLogger, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from keras.layers import Activation, BatchNormalization, concatenate, Dense, Dropout, GlobalAveragePooling2D, Input\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import cv2\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import threading\n",
    "import time\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 299\n",
    "dropout = 0.8\n",
    "batch_size = 16\n",
    "batch_size_val = 16\n",
    "\n",
    "epochs_phase_1 = 10\n",
    "epochs_phase_2 = 20\n",
    "epochs_phase_3 = 20\n",
    "\n",
    "cpu_threads = 8\n",
    "\n",
    "output_directory = './output/inceptionV3_with_dimensions_20171108'\n",
    "\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "    os.makedirs(os.path.join(output_directory, 'checkpoints'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Data and augmentaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_directory = './data2/train'\n",
    "validation_directory = './data2/validation'\n",
    "test_directory = './data2/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_seed(image, normalize=False):\n",
    "    # convert to 0-255 BGR image\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    image = cv2.convertScaleAbs(image, alpha=(255.0))\n",
    "\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    blur = cv2.GaussianBlur(gray, (15, 15), 0)\n",
    "    _, threshold = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    opening = cv2.morphologyEx(threshold, cv2.MORPH_OPEN, kernel)\n",
    "    \n",
    "    _, contours, _ = cv2.findContours(opening.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # choose contour with the largest area\n",
    "    contours.sort(key=lambda c: cv2.contourArea(c))\n",
    "    cnt = contours[-1]\n",
    "    \n",
    "    _, _, w, h = cv2.boundingRect(cnt)\n",
    "    area = cv2.contourArea(cnt)\n",
    "    perimeter = cv2.arcLength(cnt, True)\n",
    "    extent = float(area) / w * h\n",
    "    solidity = float(area) / cv2.contourArea(cv2.convexHull(cnt))\n",
    "    \n",
    "    _, size, _ = cv2.minAreaRect(cnt)\n",
    "\n",
    "    w_r = int(sorted(size)[0])\n",
    "    h_r = int(sorted(size)[1])\n",
    "    \n",
    "#     if normalize:\n",
    "#         # x_norm = (x - X_min)/(X_max - X_min)\n",
    "#         w_r = (w_r - 16)/171\n",
    "#         h_r = (h_r - 24)/266\n",
    "#         area = (area - 493)/44639\n",
    "#         perimeter = (perimeter - 86.97)/1186.53\n",
    "#         extent = (extent - 414.9)/41399.56\n",
    "    \n",
    "    return [w_r, h_r, area, perimeter, extent, solidity]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/41645042/1779687\n",
    "class batchGenerator:\n",
    "    def __init__(self, directory, generator, batch_size=16, shuffle=False):\n",
    "        gen = generator.flow_from_directory(\n",
    "            directory,\n",
    "            target_size=(input_size, input_size),\n",
    "            class_mode='categorical',\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle)\n",
    "        self.generator = gen\n",
    "        \n",
    "        self.samples = gen.samples\n",
    "        self.num_class = gen.num_class\n",
    "        self.classes = gen.classes\n",
    "        self.class_indices = gen.class_indices\n",
    "        \n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        with self.lock:\n",
    "            X, y = self.generator.next()\n",
    "            dimensions = [measure_seed(x) for x in X]\n",
    "            return [np.array(X), np.array(dimensions)], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=10,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "validgen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "testgen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = batchGenerator(\n",
    "    train_directory,\n",
    "    datagen,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True)\n",
    "\n",
    "validation_generator = batchGenerator(\n",
    "    validation_directory,\n",
    "    validgen,\n",
    "    batch_size=batch_size_val,\n",
    "    shuffle=True)\n",
    "\n",
    "test_generator = batchGenerator(\n",
    "    test_directory,\n",
    "    testgen,\n",
    "    batch_size=1,\n",
    "    shuffle=False)\n",
    "\n",
    "train_samples = train_generator.samples\n",
    "validation_samples = validation_generator.samples\n",
    "test_samples = test_generator.samples\n",
    "\n",
    "num_classes = train_generator.num_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = InceptionV3(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=(input_size, input_size, 3))\n",
    "\n",
    "x = base_model.output\n",
    "inception_out = GlobalAveragePooling2D()(x)\n",
    "\n",
    "aux_input = Input(shape=(6,), name='aux_input')\n",
    "aux = BatchNormalization(scale=False)(aux_input)\n",
    "aux = Activation('relu')(aux)\n",
    "\n",
    "x = concatenate([inception_out, aux])\n",
    "\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(dropout)(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=[base_model.input, aux_input], outputs=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Phase 1\n",
    "Train only the top layers (which were randomly initialized)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=optimizers.RMSprop(lr=1e-3),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "checkpointer = ModelCheckpoint(\n",
    "    filepath='{}/checkpoints/phase_1.h5'.format(output_directory),\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True)\n",
    "\n",
    "csv_logger = CSVLogger('{}/logs/phase_1.csv'.format(output_directory), separator=';')\n",
    "\n",
    "early_stopper = EarlyStopping(\n",
    "    monitor='val_acc',\n",
    "    min_delta=0.05,\n",
    "    patience=2,\n",
    "    verbose=1)\n",
    "\n",
    "tensorboard = TensorBoard(\n",
    "    log_dir='{}/logs/phase_1'.format(output_directory),\n",
    "    write_graph=True)\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_samples // batch_size,\n",
    "    epochs=epochs_phase_1,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_samples // batch_size_val,\n",
    "    verbose=1,\n",
    "    callbacks=[csv_logger, checkpointer, early_stopper, tensorboard],\n",
    "    workers=cpu_threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load best epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('{}/checkpoints/phase_1.h5'.format(output_directory))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Phase 2\n",
    "At this point, the top layers are well trained and we can start fine-tuning convolutional layers from inception V3. We will freeze the bottom N layers and train the remaining top layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the top 2 inception blocks\n",
    "for layer in model.layers[:249]:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[249:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=optimizers.adam(lr=1e-4),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "checkpointer = ModelCheckpoint(\n",
    "    filepath='{}/checkpoints/phase_2.h5'.format(output_directory),\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True)\n",
    "\n",
    "csv_logger = CSVLogger('{}/logs/phase_2.csv'.format(output_directory), separator=';')\n",
    "\n",
    "early_stopper = EarlyStopping(\n",
    "    monitor='val_acc',\n",
    "    min_delta=0.01,\n",
    "    patience=5,\n",
    "    verbose=1)\n",
    "\n",
    "tensorboard = TensorBoard(\n",
    "    log_dir='{}/logs/phase_2'.format(output_directory),\n",
    "    write_graph=True)\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_samples // batch_size,\n",
    "    epochs=epochs_phase_2,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_samples // batch_size_val,\n",
    "    verbose=1,\n",
    "    callbacks=[checkpointer, csv_logger, early_stopper, tensorboard],\n",
    "    workers=cpu_threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load best epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('{}/checkpoints/phase_2.h5'.format(output_directory))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Phase 3\n",
    "Train all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=optimizers.SGD(lr=1e-5, momentum=0.9, nesterov=True),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "checkpointer = ModelCheckpoint(\n",
    "    filepath='{}/checkpoints/phase_3.h5'.format(output_directory),\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True)\n",
    "\n",
    "csv_logger = CSVLogger('{}/logs/phase_3.csv'.format(output_directory), separator=';')\n",
    "\n",
    "early_stopper = EarlyStopping(\n",
    "    monitor='val_acc',\n",
    "    min_delta=0.005,\n",
    "    patience=5,\n",
    "    verbose=1)\n",
    "\n",
    "lr_reduce = ReduceLROnPlateau(verbose=1)\n",
    "\n",
    "tensorboard = TensorBoard(\n",
    "    log_dir='{}/logs/phase_3'.format(output_directory),\n",
    "    write_graph=True)\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_samples // batch_size,\n",
    "    epochs=epochs_phase_3,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_samples // batch_size_val,\n",
    "    verbose=1,\n",
    "    callbacks=[checkpointer, csv_logger, lr_reduce, early_stopper, tensorboard],\n",
    "    workers=cpu_threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load best epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('{}/checkpoints/phase_3.h5'.format(output_directory))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate_generator(test_generator, test_samples)\n",
    "\n",
    "print('Test accuracy: {:.2f}'.format(score[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Propagation time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize generator to fix imeges order\n",
    "test_generator = batchGenerator(\n",
    "    test_directory,\n",
    "    testgen,\n",
    "    batch_size=1,\n",
    "    shuffle=False)\n",
    "\n",
    "t0 = time.time()\n",
    "prediction = model.predict_generator(test_generator, test_samples)\n",
    "pred_time = time.time() - t0\n",
    "\n",
    "print('Propagation time of {} images: {:.3f} ms ({:.3f} ms per image)'.format(test_samples, pred_time * 1000.0, pred_time / test_samples * 1000.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    # print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, ha='right')\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment='center',\n",
    "                 color='white' if cm[i, j] > thresh else 'black')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [key for (key, value) in sorted(test_generator.class_indices.items())]\n",
    "\n",
    "test_pred = []\n",
    "for i in prediction:\n",
    "    test_pred.append(np.argmax(i))\n",
    "\n",
    "test_cnf_matrix = confusion_matrix(test_generator.classes, test_pred)\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "plt.figure()\n",
    "plot_confusion_matrix(test_cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix on test set')\n",
    "plt.figure()\n",
    "plot_confusion_matrix(test_cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix on test set')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
