{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "from keras.callbacks import CSVLogger, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten, Dense, Dropout, Activation, Input, merge\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers.core import  Lambda\n",
    "from keras import backend as K\n",
    "from keras.backend import set_image_data_format\n",
    "from keras.utils.layer_utils import convert_all_kernels_in_model\n",
    "\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 227\n",
    "batch_size = 16\n",
    "batch_size_val = 16\n",
    "\n",
    "epochs_phase_1 = 10\n",
    "epochs_phase_2 = 20\n",
    "epochs_phase_3 = 20\n",
    "\n",
    "cpu_threads = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Data and augmentaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_directory = './data/test'\n",
    "validation_directory = './data/test'\n",
    "test_directory = './data/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=10,\n",
    "    width_shift_range=.1,\n",
    "    height_shift_range=.1,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False,\n",
    "    fill_mode='reflect')\n",
    "\n",
    "validgen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "testgen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_directory,\n",
    "    target_size=(input_size, input_size),\n",
    "    class_mode='categorical',\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True)\n",
    "\n",
    "validation_generator = validgen.flow_from_directory(\n",
    "    validation_directory,\n",
    "    target_size=(input_size, input_size),\n",
    "    class_mode='categorical',\n",
    "    batch_size=batch_size_val,\n",
    "    shuffle=True)\n",
    "\n",
    "test_generator = testgen.flow_from_directory(\n",
    "    test_directory,\n",
    "    target_size=(input_size, input_size),\n",
    "    batch_size=1,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False)\n",
    "\n",
    "train_samples = train_generator.samples\n",
    "validation_samples = validation_generator.samples\n",
    "test_samples = test_generator.samples\n",
    "\n",
    "num_classes = train_generator.num_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on\n",
    "# https://github.com/lunardog/convnets-keras\n",
    "# https://github.com/heuritech/convnets-keras/issues/17#issuecomment-308997304"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crosschannelnormalization(alpha = 1e-4, k=2, beta=0.75, n=5, **kwargs):\n",
    "    \"\"\"\n",
    "    This is the function used for cross channel normalization in the original\n",
    "    Alexnet\n",
    "    \"\"\"\n",
    "    def f(X):\n",
    "        K.set_image_dim_ordering('th')\n",
    "        if K.backend()=='tensorflow':\n",
    "            b, ch, r, c = X.get_shape()\n",
    "        else:\n",
    "            b, ch, r, c = X.shape\n",
    "        half = n // 2\n",
    "        square = K.square(X)\n",
    "        extra_channels = K.spatial_2d_padding(K.permute_dimensions(square, (0,2,3,1))\n",
    "                                              , ((0,0),(half,half)))\n",
    "        extra_channels = K.permute_dimensions(extra_channels, (0,3,1,2))\n",
    "        scale = k\n",
    "        for i in range(n):\n",
    "            if K.backend()=='tensorflow':\n",
    "                ch = int(ch)\n",
    "            scale += alpha * extra_channels[:,i:i+ch,:,:]\n",
    "        scale = scale ** beta\n",
    "        return X / scale\n",
    "\n",
    "    return Lambda(f, output_shape=lambda input_shape:input_shape,**kwargs)\n",
    "\n",
    "def splittensor(axis=1, ratio_split=1, id_split=0, **kwargs):\n",
    "    def f(X):\n",
    "        if K.backend()=='tensorflow':\n",
    "            div = int(X.get_shape()[axis]) // ratio_split\n",
    "        else:\n",
    "            div = X.shape[axis] // ratio_split\n",
    "\n",
    "        if axis == 0:\n",
    "            output =  X[id_split*div:(id_split+1)*div,:,:,:]\n",
    "        elif axis == 1:\n",
    "            output =  X[:, id_split*div:(id_split+1)*div, :, :]\n",
    "        elif axis == 2:\n",
    "            output = X[:,:,id_split*div:(id_split+1)*div,:]\n",
    "        elif axis == 3:\n",
    "            output = X[:,:,:,id_split*div:(id_split+1)*div]\n",
    "        else:\n",
    "            raise ValueError(\"This axis is not possible\")\n",
    "\n",
    "        return output\n",
    "\n",
    "    def g(input_shape):\n",
    "        output_shape=list(input_shape)\n",
    "        output_shape[axis] = output_shape[axis] // ratio_split\n",
    "        return tuple(output_shape)\n",
    "\n",
    "    return Lambda(f,output_shape=lambda input_shape:g(input_shape),**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AlexNet(weights_path=None, num_classes=1000, input_size=227):\n",
    "    inputs = Input(shape=(3, input_size, input_size))\n",
    "\n",
    "    conv_1 = Conv2D(96, (11, 11), strides=(4, 4),activation='relu', name='conv_1')(inputs)\n",
    "\n",
    "    conv_2 = MaxPooling2D((3, 3), strides=(2,2))(conv_1)\n",
    "    conv_2 = crosschannelnormalization(name=\"convpool_1\")(conv_2)\n",
    "    conv_2 = ZeroPadding2D((2, 2))(conv_2)\n",
    "    conv_2 = merge([\n",
    "        Conv2D(128, (5, 5), activation='relu', name='conv_2_'+str(i+1))(\n",
    "            splittensor(ratio_split=2, id_split=i)(conv_2)\n",
    "        ) for i in range(2)], mode='concat', concat_axis=1, name='conv_2')\n",
    "\n",
    "    conv_3 = MaxPooling2D((3, 3), strides=(2, 2))(conv_2)\n",
    "    conv_3 = crosschannelnormalization()(conv_3)\n",
    "    conv_3 = ZeroPadding2D((1,1))(conv_3)\n",
    "    conv_3 = Conv2D(384, (3, 3), activation='relu', name='conv_3')(conv_3)\n",
    "\n",
    "    conv_4 = ZeroPadding2D((1, 1))(conv_3)\n",
    "    conv_4 = merge([\n",
    "        Conv2D(192, (3, 3), activation='relu', name='conv_4_'+str(i+1))(\n",
    "            splittensor(ratio_split=2, id_split=i)(conv_4)\n",
    "        ) for i in range(2)], mode='concat', concat_axis=1, name='conv_4')\n",
    "\n",
    "    conv_5 = ZeroPadding2D((1, 1))(conv_4)\n",
    "    conv_5 = merge([\n",
    "        Conv2D(128, (3, 3), activation='relu', name='conv_5_'+str(i+1))(\n",
    "            splittensor(ratio_split=2, id_split=i)(conv_5)\n",
    "        ) for i in range(2)], mode='concat', concat_axis=1, name='conv_5')\n",
    "\n",
    "    dense_1 = MaxPooling2D((3, 3), strides=(2,2),name=\"convpool_5\")(conv_5)\n",
    "\n",
    "    dense_1 = Flatten(name='flatten')(dense_1)\n",
    "    dense_1 = Dense(4096, activation='relu',name='dense_1')(dense_1)\n",
    "    dense_2 = Dropout(0.5)(dense_1)\n",
    "    dense_2 = Dense(4096, activation='relu',name='dense_2')(dense_2)\n",
    "    dense_3 = Dropout(0.5)(dense_2)\n",
    "    dense_3 = Dense(num_classes, name='dense_3')(dense_3)\n",
    "    predictions = Activation('softmax', name='softmax')(dense_3)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "    if weights_path:\n",
    "        model.load_weights(weights_path)\n",
    "\n",
    "        if K.backend() == 'tensorflow':\n",
    "            convert_all_kernels_in_model(model)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_image_data_format('channels_first')\n",
    "\n",
    "base_model = AlexNet(weights_path='./AlexNet/alexnet_weights.h5')\n",
    "\n",
    "# base_model.summary()\n",
    "\n",
    "x = base_model.get_layer('conv_5').output\n",
    "x = MaxPooling2D((3, 3), strides=(2,2),name=\"convpool_5\")(x)\n",
    "x = Flatten(name='flatten')(x)\n",
    "x = Dense(4096, activation='relu', name='dense_1')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(4096, activation='relu', name='dense_2')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(num_classes, name='dense_3')(x)\n",
    "predictions = Activation('softmax', name='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load weights form numpy(old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_weights = np.load(open('./AlexNet/bvlc_alexnet.npy', 'rb')).item()\n",
    "# for layer in ['conv1', 'conv2' , 'conv3' , 'conv4' , 'conv5', 'fc6', 'fc7', 'fc8']:\n",
    "#     w = net_weights[layer][0]\n",
    "#     b = net_weights[layer][1]\n",
    "#     print(w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for layer in ['conv1', 'conv2' , 'conv3' , 'conv4' , 'conv5', 'fc6', 'fc7', 'fc8']:\n",
    "#     weights = np.load(open('./AlexNet/{}w.npy'.format(layer), 'rb'))\n",
    "#     biases = np.load(open('./AlexNet/{}b.npy'.format(layer), 'rb'))\n",
    "#     print(layer)\n",
    "#     print(weights.shape)\n",
    "#     print(model.get_layer(layer).get_weights()[0].shape)\n",
    "# #     print(model.get_layer(layer).get_config())\n",
    "    \n",
    "    \n",
    "# #     model.get_layer(layer).set_weights((weights.transpose(3, 2, 1, 0), biases))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Phase 1\n",
    "Train only the top layers (which were randomly initialized)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=optimizers.RMSprop(lr=1e-3),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "checkpointer = ModelCheckpoint(\n",
    "    filepath='./output/checkpoints/phase_1_alexnet.h5',\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True)\n",
    "\n",
    "csv_logger = CSVLogger('./output/logs/phase_1_alexnet.csv', separator=';')\n",
    "\n",
    "early_stopper = EarlyStopping(\n",
    "    monitor='val_acc',\n",
    "    min_delta=0.05,\n",
    "    patience=2,\n",
    "    verbose=1)\n",
    "\n",
    "tensorboard = TensorBoard(\n",
    "    log_dir='./output/logs/phase_1_alexnet',\n",
    "    write_graph=True)\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_samples // batch_size,\n",
    "    epochs=epochs_phase_1,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_samples // batch_size_val,\n",
    "    verbose=1,\n",
    "    callbacks=[csv_logger, checkpointer, early_stopper, tensorboard],\n",
    "    workers=cpu_threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load best epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./output/checkpoints/phase_1_alexnet.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Phase 2\n",
    "Train all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=optimizers.adam(lr=1e-4),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "checkpointer = ModelCheckpoint(\n",
    "    filepath='./output/checkpoints/phase_2_alexnet.h5',\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True)\n",
    "\n",
    "csv_logger = CSVLogger('./output/logs/phase_2_alexnet.csv', separator=';')\n",
    "\n",
    "early_stopper = EarlyStopping(\n",
    "    monitor='val_acc',\n",
    "    min_delta=0.01,\n",
    "    patience=5,\n",
    "    verbose=1)\n",
    "\n",
    "tensorboard = TensorBoard(\n",
    "    log_dir='./output/logs/phase_2_alexnet',\n",
    "    write_graph=True)\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_samples // batch_size,\n",
    "    epochs=epochs_phase_2,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_samples // batch_size_val,\n",
    "    verbose=1,\n",
    "    callbacks=[checkpointer, csv_logger, early_stopper, tensorboard],\n",
    "    workers=cpu_threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load best epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./output/checkpoints/phase_2_alexnet.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate_generator(test_generator, test_samples)\n",
    "\n",
    "print('Test accuracy: {:.2f}'.format(score[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Propagation time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "prediction = model.predict_generator(test_generator, test_samples)\n",
    "pred_time = time.time() - t0\n",
    "\n",
    "print('Propagation time of {} images: {:.3f} ms ({:.3f} ms per image)'.format(test_samples, pred_time * 1000.0, pred_time / test_samples * 1000.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    # print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment='center',\n",
    "                 color='white' if cm[i, j] > thresh else 'black')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [key for (key, value) in sorted(test_generator.class_indices.items())]\n",
    "\n",
    "test_pred = []\n",
    "for i in prediction:\n",
    "    test_pred.append(np.argmax(i))\n",
    "\n",
    "test_cnf_matrix = confusion_matrix(test_generator.classes, test_pred)\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "plt.figure()\n",
    "plot_confusion_matrix(test_cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix on test set')\n",
    "plt.figure()\n",
    "plot_confusion_matrix(test_cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix on test set')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
